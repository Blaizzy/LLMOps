{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/Blaizzy/LLMOps/blob/main/inference/providers/anthropic/document_extraction.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started with Claude 3 family of LLMs\n",
    "\n",
    "<img src=\"./assets/claude 3 thumbnail.JPG\" width=500>\n",
    "\n",
    "In this guide, you'll learn how to use Anthropic's Claude 3 models to extract information from large documents, including the vision variants.\n",
    "\n",
    "Claude 3 offers a diverse range of model variants, each tailored to meet your specific requirements, ensuring optimal performance and speed:\n",
    "- **Haiku** (Fastest) ‚ö°Ô∏è: A lightning-fast model, ideal for time-sensitive tasks that demand rapid processing.\n",
    "- **Sonnet** (Performance and Speed) üöÄ: Striking the perfect balance between performance and speed, this variant excels in scenarios where both attributes are equally crucial.\n",
    "- **Opus** (Best Performance) üß†: The pinnacle of performance, this model variant is designed to tackle the most demanding and complex tasks.\n",
    "\n",
    "Additionally, Claude 3 introduces groundbreaking multimodal variants, allowing you to seamlessly integrate image and text data for better information extraction and analysis.\n",
    "\n",
    "### Essential Tools\n",
    "\n",
    "To begin your journey with Claude 3, you'll need:\n",
    "\n",
    "- **Anthropic LLM API**: Gain access to Anthropic's cutting-edge language models through their powerful API.\n",
    "- **Langchain**: Leverage this versatile library to build robust and scalable applications powered by Claude 3's capabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langchain langchain-anthropic pypdf langchain_community anthropic python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "from dotenv import load_dotenv\n",
    "from pprint import pprint\n",
    "import os\n",
    "\n",
    "# os.environ['ANTHROPIC_API_KEY'] = getpass() # create env variable directly\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Claude 3\n",
    "<img src=\"./assets/claude 3 eval.webp\" width=400>\n",
    "\n",
    "- Displays increased capabilities in analysis, content creation, code generation and low resource language understanding.\n",
    "- ~15% fewer refusals compared to previous models.\n",
    "- ~20% improvement in accuracy on challenging open-ended questions.\n",
    "- 200K context window with near-perfect recall, with up to 1M for selected users.\n",
    "- Blazing-fast processing speed with Haiku variant capable of reading 10K token papers with images in under 3 seconds.\n",
    "\n",
    "Read more <a href=\"./assets/Model_Card_Claude_3.pdf\">here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant that responds like Snoop dogg.\"),\n",
    "        (\"human\", \"{message}\")\n",
    "    ]\n",
    ")\n",
    "chat = ChatAnthropic(temperature=0, model_name=\"claude-3-sonnet-20240229\")\n",
    "chain = prompt | chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = chain.invoke({\"message\":\"Hi\"})\n",
    "pprint(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load PDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "def load_file(file_name, file_type, n_pages=None):\n",
    "    loader = PyPDFLoader(f\"./assets/{file_name}.{file_type}\")\n",
    "    pages = loader.load_and_split()\n",
    "    if n_pages:\n",
    "        return pages[:n_pages]\n",
    "    else:\n",
    "        return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = load_file(\"Model_Card_Claude_3\",\"pdf\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_prompt = [f\"<page number={i}>{page.page_content}</page>\" for i, page in enumerate(document)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human = \"\"\"\n",
    "Please carefully review the given research paper and extract the following key details, enclosing them in the specified XML tags:\n",
    "\n",
    "- The title of the research paper should be enclosed in <title></title> tags.\n",
    "- The abstract or summary of the paper should be enclosed in <abstract></abstract> tags.\n",
    "- Any mentioned models, algorithms, or techniques should be enclosed in individual <model></model> tags, with their respective capabilities or applications nested within <capabilities><capability></capability></capabilities> tags.\n",
    "- Any benchmark results for the models should be enclosed in <benchmarks></benchmarks> tags, nested within the corresponding <model></model> tags. Each benchmark should be enclosed in <benchmark></benchmark> tags, with the associated score enclosed in <score></score> tags.\n",
    "\n",
    "Example:\n",
    "<title>Research Paper Title</title>\n",
    "<abstract>This paper presents a novel approach to [...] The proposed method achieves [...] and outperforms existing techniques in terms of [...]</abstract>\n",
    "<model>\n",
    "    <name>Model A</name>\n",
    "    <capabilities>\n",
    "        <capability>Object detection</capability>\n",
    "        <capability>Image classification</capability>\n",
    "    </capabilities>\n",
    "    <benchmarks>\n",
    "        <benchmark>COCO Object Detection</benchmark>\n",
    "        <score>0.78 mAP</score>\n",
    "        <benchmark>ImageNet Classification</benchmark>\n",
    "        <score>92.5% Top-5 Accuracy</score>\n",
    "    </benchmarks>\n",
    "</model>\n",
    "<model>\n",
    "    <name>Model B</name>\n",
    "    <capabilities>\n",
    "        <capability>Efficient text summarization</capability>\n",
    "    </capabilities>\n",
    "    <benchmarks>\n",
    "        <benchmark>CNN/DailyMail Summarization</benchmark>\n",
    "        <score>41.2 ROUGE-L</score>\n",
    "    </benchmarks>\n",
    "</model>\n",
    "\n",
    "Please ensure that the extracted information accurately represents the key details from the research paper. If any of the requested details are not explicitly mentioned or available in the paper, leave the corresponding XML tags empty.\n",
    "\n",
    "<document>\n",
    "{pages}\n",
    "</document>\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant that returns XML.\"),\n",
    "        (\"human\", human)\n",
    "    ]\n",
    ")\n",
    "\n",
    "document_extraction_chain = prompt | chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = document_extraction_chain.invoke({\"pages\": pages_prompt})\n",
    "pprint(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Claude 3 Vision\n",
    "\n",
    "<img src=\"./assets/claude 3 vision eval.webp\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load image in base64\n",
    "\n",
    "Read more here:\n",
    "- https://docs.anthropic.com/claude/docs/vision\n",
    "- https://python.langchain.com/docs/integrations/chat/anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High-quality image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from pathlib import Path\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "img_path = Path(\"./assets/high_quality_img.png\")\n",
    "img_base64 = base64.b64encode(img_path.read_bytes()).decode(\"utf-8\")\n",
    "\n",
    "# Display image\n",
    "HTML(f'<img src=\"data:image/png;base64,{img_base64}\">')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "def get_image_extraction_chain(img_base64):\n",
    "    human = \"\"\"\n",
    "    Please carefully review the given image and extract the following key details, enclosing them in the specified XML tags:\n",
    "    - Any mentioned models, algorithms, or techniques should be enclosed in individual <model></model> tags.\n",
    "    - Any benchmark results for the models should be enclosed in <benchmarks></benchmarks> tags, nested within the corresponding <model></model> tags. Each benchmark should be enclosed in <benchmark></benchmark> tags, with the associated score enclosed in <score></score> tags.\n",
    "\n",
    "    Example:\n",
    "    <model>\n",
    "        <name>Model A</name>\n",
    "        <benchmarks>\n",
    "            <benchmark>COCO Object Detection</benchmark>\n",
    "            <score>0.78 mAP</score>\n",
    "            <benchmark>ImageNet Classification</benchmark>\n",
    "            <score>92.5% Top-5 Accuracy</score>\n",
    "        </benchmarks>\n",
    "    </model>\n",
    "    <model>\n",
    "        <name>Model B</name>\n",
    "        <benchmarks>\n",
    "            <benchmark>CNN/DailyMail Summarization</benchmark>\n",
    "            <score>41.2 ROUGE-L</score>\n",
    "        </benchmarks>\n",
    "    </model>\n",
    "\n",
    "    Please ensure that the extracted information accurately represents the key details from the research paper. If any of the requested details are not explicitly mentioned or available in the paper, leave the corresponding XML tags empty.\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", \"You are a helpful assistant that returns XML.\"),\n",
    "            HumanMessage(\n",
    "                content=[\n",
    "                    {\n",
    "                        \"type\":\"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/png;base64,{img_base64}\"\n",
    "                        }\n",
    "                    },\n",
    "                    {\"type\": \"text\", \"text\": \"{input}\"}\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    image_extraction_chain = prompt | chat\n",
    "    return image_extraction_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_extraction_chain = get_image_extraction_chain(img_base64)\n",
    "output = image_extraction_chain.invoke({\"input\": human})\n",
    "pprint(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low-quality image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img_path = Path(\"./assets/low_quality_img.png\")\n",
    "img_base64 = base64.b64encode(img_path.read_bytes()).decode(\"utf-8\")\n",
    "\n",
    "# Display image\n",
    "HTML(f'<img src=\"data:image/png;base64,{img_base64}\">')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_extraction_chain = get_image_extraction_chain(img_base64)\n",
    "output = image_extraction_chain.invoke({\"input\": human})\n",
    "pprint(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function Calling and Tools:\n",
    "- https://docs.anthropic.com/claude/docs/functions-external-tools\n",
    "- https://python.langchain.com/docs/integrations/chat/anthropic_functions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

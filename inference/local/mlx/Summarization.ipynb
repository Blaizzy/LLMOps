{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/Blaizzy/LLMOps/blob/main/inference/local/mlx/Summarization.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started with Gemma Google's lightweight family of LLMs\n",
    "\n",
    "<img src=\"./assets/gemma_logo.webp\" width=500>\n",
    "\n",
    "In this guide, you'll learn how to use Google's Gemma 2B and 7B to summarise large documents.\n",
    "\n",
    "### Tools\n",
    "- Huggingface\n",
    "- MLX\n",
    "- Langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U -q langchain pypdf langchain_community tqdm\n",
    "!pip install -U -q mlx-lm # For MacBook\n",
    "!pip install -U -q huggingface-hub hf-transfer\n",
    "# !pip install -U -q transformers accelerate bitsandbytes huggingface-hub hf-transfer # For GPU accelerated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm import load, generate\n",
    "import os\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "model_id = \"mlx-community/quantized-gemma-2b-it\"\n",
    "model, tokenizer = load(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jinja2 import Template\n",
    "from typing import List\n",
    "from pprint import pprint\n",
    "\n",
    "def apply_chat_template(messages:List, add_generation:bool=True):\n",
    "    template_str = \"{% for item in messages %}\" \\\n",
    "                   \"<start_of_turn>{{ item.role }}\\n{{ item.content }}<end_of_turn>\\n\" \\\n",
    "                   \"{% if loop.last %}{% if add_generation %}<start_of_turn>model\\n{% endif %}{% endif %}\" \\\n",
    "                   \"{% endfor %}\"\n",
    "\n",
    "    template = Template(template_str)\n",
    "    result = template.render(messages=messages, add_generation=add_generation)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"content\": \"Respond like snoop dogg. Who is Einstein?\", \"role\":\"user\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generate(model, tokenizer, prompt=apply_chat_template(messages), temp=0.1, max_tokens=500, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def load_file(file_name, file_type):\n",
    "    loader = PyPDFLoader(f\"./assets/{file_name}.{file_type}\")\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        # Set a really small chunk size, just to show.\n",
    "        chunk_size=5000,\n",
    "        chunk_overlap=20,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    return loader.load_and_split(text_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = load_file('gemma-report', 'pdf')\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Create a summary of the following document:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "prompt = \"Create a summary of the following document:\"\n",
    "summaries = []\n",
    "\n",
    "\n",
    "for doc in tqdm(documents[:9]):\n",
    "    summaries.append(\n",
    "        generate(\n",
    "            model,\n",
    "            tokenizer=tokenizer,\n",
    "            prompt=apply_chat_template([\n",
    "                {\"content\": prompt + f\"'{doc.page_content}'\", \"role\": \"user\"}\n",
    "            ]),\n",
    "            temp=0.1,\n",
    "            max_tokens=500,\n",
    "            verbose=True\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(\"\\n\".join(summaries)) # No. Characters ~ 2621 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_text = \"\\n\".join(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, tokenizer # Free up resources to run a bigger model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"mlx-community/quantized-gemma-7b-it\"\n",
    "model, tokenizer = load(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "prompt_for_long_summary = {\"content\": f\"Give me a summary of the following document: '{summaries_text}'\", \"role\":\"user\"}\n",
    "final_summary = generate(model, tokenizer=tokenizer, prompt=apply_chat_template([prompt_for_long_summary]), temp=0.1, max_tokens=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(final_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLX-LM server\n",
    "You can host your model's locally with a OpenAI compatible API using MLX-LM server.\n",
    "\n",
    "Command to run server:\n",
    "\n",
    "`python -m mlx_lm.server --model 'mlx-community/quantized-gemma-2b-it'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"http://localhost:8080/v1/chat/completions\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "data = {\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Hi\"}],\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_tokens\": 100,\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    response = result['choices'][0]['message']['content'].replace(\"<eos>\", \"\")\n",
    "    print(f\"Response:\\n{response}\")\n",
    "    print(\"===========\")\n",
    "    print(f\"Usage:\\n{result['usage']}\")\n",
    "else:\n",
    "    print(f\"Request failed with status code: {response.status_code}\")\n",
    "    print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl localhost:8080/v1/chat/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"messages\": [{\"role\": \"user\", \"content\": \"Say this is a test!\"}],\"temperature\": 0.7}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-ft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

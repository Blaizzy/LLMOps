{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U mlx-vlm gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx\n",
    "from mlx_vlm import load, generate\n",
    "from mlx_vlm.utils import generate_step, sample, prepare_inputs, load_config, load_image_processor, get_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c8d23cf4bbc45cca6bdcd590fcf8128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_path = \"mlx-community/nanoLLaVA\"\n",
    "model_path = get_model_path(model_path)\n",
    "model, processor = load(model_path)\n",
    "config = load_config(model_path)\n",
    "image_processor = load_image_processor(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Image: ./assets/image.png \n",
      "\n",
      "Prompt: <|im_start|>system\n",
      "Answer the questions.<|im_end|><|im_start|>user\n",
      "<image>\n",
      "What's so funny about this image?<|im_end|><|im_start|>assistant\n",
      "\n",
      "This image is quite amusing. It's a humorous drawing of a man holding a large, yellow, and black balloon. The man is wearing a white shirt and has a white beard. The balloon is so large that it almost covers the man's entire body. The man's hand is also visible, and it seems like he's holding the balloon with both hands.\n",
      "==========\n",
      "Prompt: 77.345 tokens-per-sec\n",
      "Generation: 46.756 tokens-per-sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"This image is quite amusing. It's a humorous drawing of a man holding a large, yellow, and black balloon. The man is wearing a white shirt and has a white beard. The balloon is so large that it almost covers the man's entire body. The man's hand is also visible, and it seems like he's holding the balloon with both hands.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = processor.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": f\"<image>\\nWhat's so funny about this image?\"}],\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "output = generate(model, processor, \"./assets/image.png\", prompt, image_processor, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import Optional\n",
    "\n",
    "def generate(\n",
    "    model,\n",
    "    processor,\n",
    "    image: str,\n",
    "    prompt: str,\n",
    "    image_processor = None,\n",
    "    temp: float = 0.0,\n",
    "    max_tokens: int = 100,\n",
    "    repetition_penalty: Optional[float] = None,\n",
    "    repetition_context_size: Optional[int] = None,\n",
    "    top_p: float = 1.0,\n",
    "):\n",
    "\n",
    "    if image_processor is not None:\n",
    "        tokenizer = processor\n",
    "    else:\n",
    "        tokenizer = processor.tokenizer\n",
    "\n",
    "    input_ids, pixel_values = prepare_inputs(image_processor, processor, image, prompt)\n",
    "    logits, cache = model(input_ids, pixel_values)\n",
    "    logits = logits[:, -1, :]\n",
    "    y, _ = sample(logits, temp, top_p)\n",
    "\n",
    "\n",
    "    tic = time.perf_counter()\n",
    "    detokenizer = processor.detokenizer\n",
    "    detokenizer.reset()\n",
    "\n",
    "    detokenizer.add_token(y.item())\n",
    "\n",
    "    for (token, _), n in zip(\n",
    "        generate_step(\n",
    "            model.language_model,\n",
    "            logits,\n",
    "            cache,\n",
    "            temp,\n",
    "            repetition_penalty,\n",
    "            repetition_context_size,\n",
    "            top_p,\n",
    "        ),\n",
    "        range(max_tokens),\n",
    "    ):\n",
    "        token = token.item()\n",
    "\n",
    "        if token == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "        detokenizer.add_token(token)\n",
    "        detokenizer.finalize()\n",
    "        yield detokenizer.last_segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7869\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def generate_response(message, history):\n",
    "    prompt = message[\"text\"]\n",
    "\n",
    "    prompt = processor.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": f\"<image>\\n{prompt}\"}],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    response = \"\"\n",
    "    for chunk in generate(model, processor, message[\"files\"][0], prompt, image_processor):\n",
    "        response+=chunk\n",
    "        yield response\n",
    "\n",
    "demo = gr.ChatInterface(fn=generate_response, title=\"MLX-VLM Bot\", multimodal=True)\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx_code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

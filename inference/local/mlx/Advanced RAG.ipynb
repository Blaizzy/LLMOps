{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "chat-with-mlx 0.1.11 requires langchain==0.1.16, but you have langchain 0.2.5 which is incompatible.\n",
      "chat-with-mlx 0.1.11 requires langchain-core==0.1.45, but you have langchain-core 0.2.7 which is incompatible.\n",
      "transformers 4.41.0.dev0 requires tokenizers<0.20,>=0.19, but you have tokenizers 0.15.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cohere 5.5.7 requires tokenizers<0.16,>=0.15, but you have tokenizers 0.19.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U -q mlx-lm\n",
    "!pip install -U -q langchain_community langchain-cohere langchain\n",
    "!pip install -U -q pypdf faiss-cpu python-dotenv defusedxml tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_community.chat_models.mlx import ChatMLX\n",
    "from langchain_community.llms.mlx_pipeline import MLXPipeline\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain.output_parsers import XMLOutputParser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "from langchain.schema import (\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    AIMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b23123d4db344b119e51284d299cebaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "router_llm = MLXPipeline.from_model_id(\n",
    "    \"mlx-community/Mistral-7B-Instruct-v0.3-4bit\",\n",
    "    pipeline_kwargs={\"max_tokens\": 200, \"temp\": 0.1},\n",
    ")\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"retriever\",\n",
    "            \"description\": \"Useful for retrieving factual documents and information from a database or API about a user's request to answer their queries.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"user_query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The query to retrieve and ground information for.\"\n",
    "                    },\n",
    "\n",
    "                },\n",
    "                \"required\": [\"user_query\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"direct_response\",\n",
    "            \"description\": \"Useful for providing a direct response without retrieving additional information.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"user_query\": {\n",
    "                        \"type\": \"string\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"user_query\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"vision\",\n",
    "            \"description\": \"Useful for analysing images.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"user_query\": {\n",
    "                        \"type\": \"string\",\n",
    "                    },\n",
    "                    \"image_path\": {\n",
    "                        \"type\": \"string\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"user_query\", \"image_path\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\" You are Mistral with function-calling supported. You are provided with function signatures within <tools></tools> XML tags.\n",
    "    You may call one function to assist with the user query. Don't make assumptions about what values to plug into functions.\n",
    "    Here are the available tools:\n",
    "    <tools>\n",
    "    {tools}\n",
    "    </tools>\n",
    "\n",
    "    For each function call, return a XML object with the function name and arguments within <tool_call></tool_call> XML tags as follows:\n",
    "    <tool_call>\n",
    "        <function_name></function_name>\n",
    "        <arguments>\n",
    "            <name></name>\n",
    "            <value></value>\n",
    "            <image_path></image_path>\n",
    "        </arguments>\n",
    "    </tool_call>\n",
    "\n",
    "    If the user question requires factual answers, use the retriever tool only.\n",
    "\n",
    "    <tool_call>\n",
    "        <function_name>retriever</function_name>\n",
    "        <arguments>\n",
    "            <name></name>\n",
    "            <value></value>\n",
    "        </arguments>\n",
    "    </tool_call>\n",
    "\n",
    "    Please respond using XML ONLY.\n",
    "    Question: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "router = ChatMLX(llm=router_llm)\n",
    "\n",
    "router_chain = (\n",
    "    {\"tools\": itemgetter(\"tools\"), \"question\": RunnablePassthrough(), \"image\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | router\n",
    "    | XMLOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tool_call': [{'function_name': 'direct_response'},\n",
       "  {'arguments': [{'name': 'user_query'}, {'value': 'Hi, how are you??'}]}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "router_output = router_chain.invoke({\"tools\": tools, \"question\": \"Hi, how are you??\"})\n",
    "router_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tool_call': [{'function_name': 'vision'},\n",
       "  {'arguments': [{'name': 'user_query'},\n",
       "    {'value': \"Caption this image './assets/training_pipelines.jpg'\"},\n",
       "    {'name': 'image_path'},\n",
       "    {'value': './assets/training_pipelines.jpg'}]}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "router_output = router_chain.invoke({\"tools\": tools, \"question\": \"Caption this image './assets/training_pipelines.jpg'\"})\n",
    "router_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tool_call': [{'function_name': 'retriever'},\n",
       "  {'arguments': [{'name': 'user_query'},\n",
       "    {'value': 'Explain how Gemma model works'}]}]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "router_output = router_chain.invoke({\"tools\": tools, \"question\": \"Explain how Gemma model works?\", \"image\": \"\"})\n",
    "router_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/mlx_code/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # take environment variables from .env.\n",
    "\n",
    "def load_file(file_name, file_type):\n",
    "    loader = PyPDFLoader(f\"./assets/{file_name}.{file_type}\")\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        # Set a really small chunk size, just to show.\n",
    "        chunk_size=5000,\n",
    "        chunk_overlap=20,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    return loader.load_and_split(text_splitter)\n",
    "\n",
    "\n",
    "documents = load_file('gemma-report', 'pdf')\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents, embedding=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Gemma: Open Models Based on Gemini Research and Technology\\nContributions and Acknowledgments\\nCore Contributors\\nThomas Mesnard\\nCassidy Hardin\\nRobert Dadashi\\nSurya Bhupatiraju\\nShreya Pathak\\nLaurent Sifre\\nMorgane Rivière\\nMihir Sanjay Kale\\nJuliette Love\\nPouya Tafti\\nLéonard Hussenot\\nContributors\\nAakanksha Chowdhery\\nAdam Roberts\\nAditya Barua\\nAlex Botev\\nAlex Castro-Ros\\nAmbrose Slone\\nAmélie Héliou\\nAndrea Tacchetti\\nAnna Bulanova\\nAntonia Paterson\\nBeth Tsai\\nBobak Shahriari\\nCharline Le Lan\\nChristopher Choquette\\nClément Crepy\\nDaniel Cer\\nDaphne Ippolito\\nDavid Reid\\nElena Buchatskaya\\nEric Ni\\nEric Noland\\nGeng Yan\\nGeorge Tucker\\nGeorge-Christian Muraru\\nGrigory Rozhdestvenskiy\\nHenryk Michalewski\\nIan Tenney\\nIvan Grishchenko\\nJacob Austin\\nJames Keeling\\nJane Labanowski\\nJean-Baptiste Lespiau\\nJeff Stanway\\nJenny BrennanJeremy Chen\\nJohan Ferret\\nJustin Chiu\\nJustin Mao-Jones\\nKatherine Lee\\nKathy Yu\\nKatie Millican\\nLars Lowe Sjoesund\\nLisa Lee\\nLucas Dixon\\nMachel Reid\\nMaciej Mikuła\\nMateo Wirth\\nMichael Sharman\\nNikolai Chinaev\\nNithum Thain\\nOlivier Bachem\\nOscar Chang\\nOscar Wahltinez\\nPaige Bailey\\nPaul Michel\\nPetko Yotov\\nPier Giuseppe Sessa\\nRahma Chaabouni\\nRamona Comanescu\\nReena Jana\\nRohan Anil\\nRoss McIlroy\\nRuibo Liu\\nRyan Mullins\\nSamuel L Smith\\nSebastian Borgeaud\\nSertan Girgin\\nSholto Douglas\\nShree Pandya\\nSiamak Shakeri\\nSoham De\\nTed Klimenko\\nTom Hennigan\\nVlad Feinberg\\nWojciech Stokowiec\\nYu-hui Chen\\nZafarali Ahmed\\nZhitao Gong\\n11', metadata={'source': './assets/gemma-report.pdf', 'page': 10}),\n",
       " Document(page_content='Gemma: Open Models Based on Gemini Research and Technology\\nLLaMA-2 Mistral Gemma\\nBenchmark metric 7B 13B 7B 2B 7B\\nMMLU 5-shot, top-1 45.3 54.8 62.5 42.3 64.3\\nHellaSwag 0-shot 77.2 80.7 81.0 71.4 81.2\\nPIQA 0-shot 78.8 80.5 82.2 77.3 81.2\\nSIQA 0-shot 48.3 50.3 47.0∗49.751.8\\nBoolq 0-shot 77.4 81.7 83.2∗69.483.2\\nWinogrande partial scoring 69.2 72.8 74.2 65.4 72.3\\nCQA 7-shot 57.8 67.3 66.3∗65.371.3\\nOBQA 58.657.0 52.2 47.8 52.8\\nARC-e 75.2 77.3 80.5 73.2 81.5\\nARC-c 45.9 49.4 54.9 42.1 53.2\\nTriviaQA 5-shot 72.1 79.6 62.5 53.2 63.4\\nNQ 5-shot 25.7 31.2 23.2 12.5 23.0\\nHumanEval pass@1 12.8 18.3 26.2 22.0 32.3\\nMBPP†3-shot 20.8 30.6 40.2∗29.244.4\\nGSM8K maj@1 14.6 28.7 35.4∗17.746.4\\nMATH 4-shot 2.5 3.9 12.7 11.8 24.3\\nAGIEval 29.3 39.1 41.2∗24.241.7\\nBBH 32.6 39.4 56.1∗35.2 55.1\\nAverage 47.0 52.2 54.0 44.9 56.4\\nTable 6|Academic benchmark results, compared to similarly sized, openly-available models trained\\non general English text data.†Mistral reports 50.2 on a different split for MBPP and on their split\\nour 7B model achieves 54.5.∗evaluations run by us. Note that due to restrictive licensing, we were\\nunable to run evals on LLaMA-2; all values above were previously reported in Touvron et al. (2023b).\\nmodel to exceed this threshold, there is signifi-\\ncantroomforcontinuedimprovementstoachieve\\nGemini and human-level performance.\\nGemma models demonstrate particularly\\nstrong performance on mathematics and coding\\nbenchmarks. On mathematics tasks, which\\nare often used to benchmark the general ana-\\nlytical capabilities of models, Gemma models\\noutperform other models by at least 10 points\\non GSM8K (Cobbe et al., 2021) and the more\\ndifficult MATH (Hendrycks et al., 2021) bench-\\nmark. Similarly, they outperform alternate open\\nmodels by at least 6 points on HumanEval (Chen\\net al., 2021). They even surpass the performance\\nof the code-fine-tuned CodeLLaMA-7B models\\non MBPP (CodeLLaMA achieves a score of 41.4%\\nwhere Gemma 7B achieves 44.4%).Memorization Evaluations\\nRecent work has shown that aligned models may\\nbe vulnerable to new adversarial attacks that can\\nbypass alignment (Nasr et al., 2023). These at-\\ntackscancausemodelstodiverge,andsometimes\\nregurgitate memorized training data in the pro-\\ncess. We focus on discoverable memorization,\\nwhich serves as a reasonable upper-bound on the\\nmemorization of a model (Nasr et al., 2023) and\\nhas been the common definition used in several\\nstudies (Anil et al., 2023; Carlini et al., 2022;\\nKudugunta et al., 2023).\\nWe test for memorization1of the Gemma pre-\\ntrained models with the same methodology per-\\nformed in Anil et al. (2023). We sample 10,000\\n1Our use of “memorization” relies on the definition of\\nthat term found at www.genlaw.org/glossary.html.\\n6', metadata={'source': './assets/gemma-report.pdf', 'page': 5}),\n",
       " Document(page_content='Gemma: Open Models Based on Gemini Research and Technology\\nProduct Management\\nTris Warkentin\\nLudovic Peran\\nProgram Management\\nMinh Giang\\nExecutive Sponsors\\nClement Farabet\\nOriol Vinyals\\nJeff Dean\\nKoray Kavukcuoglu\\nDemis Hassabis\\nZoubin Ghahramani\\nDouglas Eck\\nJoelle Barral\\nFernando Pereira\\nEli Collins\\nLeads\\nArmand Joulin\\nNoah Fiedel\\nEvan Senter\\nTech Leads\\nAlek Andreev†\\nKathleen Kenealy†\\nAcknowledgements\\nOur work is made possible by the dedication and\\nefforts of numerous teams at Google. We would\\nlike to acknowledge the support from the fol-\\nlowing teams: Gemini, Gemini Safety, Gemini\\nInfrastructure, Google Cloud, Google Research\\nResponsible AI, Kaggle, Keras.\\nSpecial thanks and acknowledgment to Adrian\\nHutter, Andreas Terzis, Andrei Kulik, Angelos Fi-\\nlos, Anushan Fernando, Aurelien Boffy, Clement\\nCrepy, Danila Sinopalnikov, Edouard Leurent,\\nGabriela Surita, Geoffrey Cideron, Jilin Chen,\\nKarthik Raveendran, Kathy Meier-Hellstern, Ke-\\nhang Han, Kevin Robinson, Kritika Muralidha-\\nran, Le Hou, Leonard Berrada, Lev Proleev, Marie\\nPellat, Mark Sherwood, Matt Hoffman, Matthias\\nGrundmann, Nicola De Cao, Nikola Momchev,\\nNino Vieillard, Noah Constant, Peter Liu, Piotr\\nStanczyk, Qiao Zhang, Ruba Haroun, Seliem El-\\nSayed, Siddhartha Brahma, Tianhe (Kevin) Yu,\\nTom Le Paine, Yingjie Miao, Yuanzhong Xu, and\\nYuting Sun.\\n†equal contribution.References\\nE. Almazrouei, H. Alobeidli, A. Alshamsi, A. Cap-\\npelli, R. Cojocaru, M. Debbah, Étienne Goffinet,\\nD. Hesslow, J. Launay, Q. Malartic, D. Mazzotta,\\nB. Noune, B. Pannier, and G. Penedo. The fal-\\ncon series of open language models, 2023.\\nD. Amodei, C. Olah, J. Steinhardt, P. Christiano,\\nJ. Schulman, and D. Mané. Concrete problems\\nin AI safety. arXiv preprint , 2016.\\nR. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lep-\\nikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey,\\nZ. Chen, et al. Palm 2 technical report. arXiv\\npreprint arXiv:2305.10403 , 2023.\\nJ. Austin, A. Odena, M. I. Nye, M. Bosma,\\nH. Michalewski, D. Dohan, E. Jiang, C. J.\\nCai, M. Terry, Q. V. Le, and C. Sutton. Pro-\\ngram synthesis with large language models.\\nCoRR, abs/2108.07732, 2021. URL https:\\n//arxiv.org/abs/2108.07732 .\\nY.Bai,S.Kadavath,S.Kundu,A.Askell,J.Kernion,\\nA. Jones, A. Chen, A. Goldie, A. Mirhoseini,\\nC. McKinnon, C. Chen, C. Olsson, C. Olah,\\nD. Hernandez, D. Drain, D. Ganguli, D. Li,\\nE. Tran-Johnson, E. Perez, J. Kerr, J. Mueller,\\nJ. Ladish, J. Landau, K. Ndousse, K. Lukosuite,\\nL. Lovitt, M. Sellitto, N. Elhage, N. Schiefer,\\nN. Mercado, N. DasSarma, R. Lasenby, R. Lar-\\nson, S. Ringer, S. Johnston, S. Kravec, S. E.\\nShowk, S. Fort, T. Lanham, T. Telleen-Lawton,\\nT. Conerly, T. Henighan, T. Hume, S. R. Bow-\\nman, Z. Hatfield-Dodds, B. Mann, D. Amodei,\\nN. Joseph, S. McCandlish, T. Brown, and J. Ka-\\nplan. Constitutional ai: Harmlessness from ai\\nfeedback, 2022.\\nP. Barham, A. Chowdhery, J. Dean, S. Ghemawat,\\nS. Hand, D. Hurt, M. Isard, H. Lim, R. Pang,\\nS. Roy, B. Saeta, P. Schuh, R. Sepassi, L. E.\\nShafey, C. A. Thekkath, and Y. Wu. Path-\\nways: Asynchronous distributed dataflow for\\nml, 2022.\\nY. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi.\\nPIQA: reasoning about physical commonsense\\nin natural language. CoRR, abs/1911.11641,\\n2019. URL http://arxiv.org/abs/1911.\\n11641.\\n12', metadata={'source': './assets/gemma-report.pdf', 'page': 11}),\n",
       " Document(page_content='Gemma: Open Models Based on Gemini Research and Technology\\nPerformance by Score\\n020406080\\nQuestion Answering Reasoning Math / Science CodingLLaMA 2 (7B) LLaMA 2 (13B) Mistral (7B) Gemma (7B)\\nFigure 1|Language understanding and generation performance of Gemma 7B across different capa-\\nbilities compared to similarly sized open models. We group together standard academic benchmark\\nevaluations by capability and average the respective scores; see Table 6 for a detailed breakdown of\\nperformance.\\nthe development of the next wave of innovations.\\nWhile thorough testing of all Gemma models has\\nbeen conducted, testing cannot cover all appli-\\ncations and scenarios in which Gemma may be\\nused. With this in mind, all Gemma users should\\nconduct rigorous safety testing specific to their\\nuse case before deployment or use. More details\\non our approach to safety can be found in section\\nResponsible Deployment.\\nIn this technical report, we provide a detailed\\noverview of the model architecture, training in-\\nfrastructure, and pretraining and fine-tuning\\nrecipes for Gemma, followed by thorough eval-\\nuations of all checkpoints across a wide-variety\\nof quantitative and qualitative benchmarks, as\\nwell as both standard academic benchmarks and\\nhuman-preference evaluations. We then discuss\\nin detail our approach to safe and responsible de-\\nployment. Finally, we outline the broader impli-\\ncations of Gemma, its limitations and advantages,\\nand conclusions.\\nModel Architecture\\nThe Gemma model architecture is based on the\\ntransformer decoder (Vaswani et al., 2017). The\\ncore parameters of the architecture are summa-Parameters 2B 7B\\nd_model 2048 3072\\nLayers 18 28\\nFeedforward hidden dims 32768 49152\\nNum heads 8 16\\nNum KV heads 1 16\\nHead size 256 256\\nVocab size 256128 256128\\nTable 1|Key model parameters.\\nModelEmbedding\\nParametersNon-embedding\\nParameters\\n2B 524,550,144 1,981,884,416\\n7B 786,825,216 7,751,248,896\\nTable 2|Parameter counts for both sizes of\\nGemma models.\\nrized in Table 1. Models are trained on a context\\nlength of 8192 tokens.\\nWe also utilize several improvements proposed\\nafter the original transformer paper. Below, we\\nlist the included improvements:\\nMulti-Query Attention (Shazeer, 2019). No-\\n2', metadata={'source': './assets/gemma-report.pdf', 'page': 1})]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"Gemma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87bfa6a22c244ae287ea8468e1eed087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "writer_llm = MLXPipeline.from_model_id(\n",
    "    \"mlx-community/Qwen2-0.5B-Instruct\",\n",
    "    pipeline_kwargs={\"max_tokens\": 100, \"temp\": 0.7},\n",
    ")\n",
    "writer = ChatMLX(llm=writer_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tool calling + RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0fcc1579ca14b40aa46f93d2374e47f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "970f6ee0be024fd69368290eaae8704e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mlx_vlm.utils import load, generate, load_image_processor\n",
    "from mlx_vlm.prompt_utils import get_message_json\n",
    "\n",
    "model_path = \"mlx-community/deepseek-vl-7b-chat-4bit\"\n",
    "vision_model, processor = load(model_path)\n",
    "image_processor = load_image_processor(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    formatted_docs = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        formatted_doc = f\"<doc id='{i}'>{doc.page_content}</doc>\"\n",
    "        formatted_docs.append(formatted_doc)\n",
    "    return \"\\n\".join(formatted_docs)\n",
    "\n",
    "\n",
    "def create_retriever_chain():\n",
    "    template = \"\"\"Answer the question based only on the following context:\n",
    "            {context}\n",
    "\n",
    "            Question: {question}\n",
    "        \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    chain = (\n",
    "        {\n",
    "            \"context\": RunnablePassthrough(),\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "        | prompt\n",
    "        | writer\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return chain\n",
    "\n",
    "\n",
    "def tool_call(function_name, arguments):\n",
    "    question = arguments[-1]['value']\n",
    "    if function_name == 'get_weather':\n",
    "\n",
    "        return \"The weather is good!\"\n",
    "\n",
    "    if function_name == 'retriever':\n",
    "        retriever_chain = create_retriever_chain()\n",
    "        docs = format_docs(retriever.invoke(question))\n",
    "        output = retriever_chain.invoke({\"context\": docs, \"question\": question})\n",
    "        return output\n",
    "\n",
    "    if function_name == 'vision':\n",
    "        question = arguments[1]['value']\n",
    "        image_path = arguments[-1]['value']\n",
    "        print(question, image_path)\n",
    "        prompt = processor.apply_chat_template(\n",
    "            [get_message_json(vision_model.config.model_type, question)],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "        output = generate(\n",
    "            vision_model,\n",
    "            processor,\n",
    "            image_path,\n",
    "            prompt,\n",
    "            image_processor,\n",
    "            temp=0.7,\n",
    "            max_tokens=100,\n",
    "            verbose=False\n",
    "        )\n",
    "        return output\n",
    "\n",
    "    else:\n",
    "        messages = [\n",
    "            HumanMessage(\n",
    "                content=question\n",
    "            ),\n",
    "        ]\n",
    "        output = writer.invoke(messages)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_name= router_output[\"tool_call\"][0][\"function_name\"]\n",
    "args = router_output[\"tool_call\"][-1][\"arguments\"]\n",
    "tool_call_out = tool_call(function_name, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool: retriever\n",
      "Question: Explain how Gemma model works\n",
      "Answer: The Gemma model is a large language model (LLM) developed by Google. It is a Transformer-based model that is trained on a context length of 8192 tokens. The model is based on the Transformer decoder and includes a feedforward hidden layer with 32768 filters and 49152 heads. The vocabulary size is 256128, and the number of KV heads is 16. The model has 256\n"
     ]
    }
   ],
   "source": [
    "print(\"Tool:\", function_name)\n",
    "print(\"Question:\", args[-1][\"value\"])\n",
    "print(\"Answer:\",tool_call_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "def rag_pipeline(question):\n",
    "    router_output = router_chain.invoke({\"tools\": tools, \"question\": question})\n",
    "    function_name= router_output[\"tool_call\"][0][\"function_name\"]\n",
    "    args = router_output[\"tool_call\"][-1][\"arguments\"]\n",
    "    tool_call_out = tool_call(function_name, args)\n",
    "    print(\"Tool:\", function_name)\n",
    "    print(\"Question:\", args[-1][\"value\"])\n",
    "    print(\"=============\\nAnswer:\")\n",
    "\n",
    "    pprint(tool_call_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool: retriever\n",
      "Question: How many sizes does Gemma have?\n",
      "=============\n",
      "Answer:\n",
      "'Gemma has a total of 6 sizes.'\n"
     ]
    }
   ],
   "source": [
    "rag_pipeline(\"How many sizes does gemma have?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG + VISION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Describe each stage of this image: './assets/training_pipelines.jpg' ./assets/training_pipelines.jpg\n",
      "Tool: vision\n",
      "Question: ./assets/training_pipelines.jpg\n",
      "=============\n",
      "Answer:\n",
      "('The 1: Training VL Adapter\\n'\n",
      " 'In this stage, the VL Adapter is trained using a mixture of image-text pairs '\n",
      " 'and pure language sequences. The model, DeepSeek LM, is responsible for '\n",
      " 'processing the input data and generating the outputs required for the '\n",
      " 'subsequent stages.\\n'\n",
      " '\\n'\n",
      " 'Stage 2: Joint VL Pro-training\\n'\n",
      " 'In this stage, the VL Adapter is jointly pre-trained with the '\n",
      " 'vision-and-language model, DeepSeek ViT. The training process involves using '\n",
      " 'a mixture of image')\n"
     ]
    }
   ],
   "source": [
    "rag_pipeline(\"Describe each stage of this image: './assets/training_pipelines.jpg'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx_code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install mlx-lm\n",
    "!pip install -q langchain-cohere  langchain pypdf faiss-cpu python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ Langchain + MLX integration is still under review.\n",
    "\n",
    "You can check the progress here: https://github.com/langchain-ai/langchain/pull/18152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q git+https://github.com/Blaizzy/langchain.git@pc/mlx#subdirectory=libs/community --use-pep517"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_community.chat_models.mlx import ChatMLX\n",
    "from langchain_community.llms.mlx_pipeline import MLXPipeline\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain.output_parsers import XMLOutputParser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "from langchain.schema import (\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    AIMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81d8b36b48144df88db4e3134f2f3e1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "router_llm = MLXPipeline.from_model_id(\n",
    "    \"mlx-community/Mistral-7B-Instruct-v0.2-4bit\",\n",
    "    pipeline_kwargs={\"max_tokens\": 200, \"temp\": 0.1},\n",
    ")\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"retriever\",\n",
    "            \"description\": \"Useful for retrieving factual documents and information from a database or API about a user's request to answer their queries.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"user_query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The query to retrieve and ground information for.\"\n",
    "                    },\n",
    "\n",
    "                },\n",
    "                \"required\": [\"user_query\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"direct_response\",\n",
    "            \"description\": \"Useful for providing a direct response without retrieving additional information.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"user_query\": {\n",
    "                        \"type\": \"string\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"user_query\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\" You are Mistral with function-calling supported. You are provided with function signatures within <tools></tools> XML tags.\n",
    "    You may call one function to assist with the user query. Don't make assumptions about what values to plug into functions.\n",
    "    Here are the available tools:\n",
    "    <tools>\n",
    "    {tools}\n",
    "    </tools>\n",
    "\n",
    "    For each function call, return a XML object with the function name and arguments within <tool_call></tool_call> XML tags as follows:\n",
    "    <tool_call>\n",
    "        <function_name></function_name>\n",
    "        <arguments></arguments>\n",
    "    </tool_call>\n",
    "\n",
    "    If the user question requires factual answers, use the retriever tool only.\n",
    "\n",
    "    <tool_call>\n",
    "        <function_name>retriever</function_name>\n",
    "        <arguments>\n",
    "        </arguments>\n",
    "    </tool_call>\n",
    "\n",
    "    Please respond using XML ONLY.\n",
    "\n",
    "\n",
    "    Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "router = ChatMLX(llm=router_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "router_chain = (\n",
    "    {\"tools\": itemgetter(\"tools\"), \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | router\n",
    "    | XMLOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "router_output = router_chain.invoke({\"tools\": tools, \"question\": \"What's the weather like in Maputo?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response': [{'tool_call': [{'function_name': 'retriever'},\n",
       "    {'arguments': [{'user_query': \"What's the weather like in Maputo?\"}]}]}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "router_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "router_output = router_chain.invoke({\"tools\": tools, \"question\": \"Hi, how are you??\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Root': [{'tool_call': [{'function_name': 'direct_response'},\n",
       "    {'arguments': [{'user_query': 'Hi, how are you?'}]}]}]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "router_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "router_output = router_chain.invoke({\"tools\": tools, \"question\": \"Explain how Gemma model works?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response': [{'tool_call': [{'function_name': 'retriever'},\n",
       "    {'arguments': [{'user_query': 'Explain how Gemma model works'}]}]}]}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "router_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # take environment variables from .env.\n",
    "\n",
    "def load_file(file_name, file_type):\n",
    "    loader = PyPDFLoader(f\"./assets/{file_name}.{file_type}\")\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        # Set a really small chunk size, just to show.\n",
    "        chunk_size=5000,\n",
    "        chunk_overlap=20,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    return loader.load_and_split(text_splitter)\n",
    "\n",
    "\n",
    "documents = load_file('gemma-report', 'pdf')\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents, embedding=CohereEmbeddings(model=\"embed-english-light-v3.0\")\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Gemma: Open Models Based on Gemini Research and Technology\\ndevelopment ecosystem will enable downstream\\ndevelopers to create a host of beneficial appli-\\ncations, in areas such as science, education and\\nthe arts. Our instruction-tuned offerings should\\nencourage a range of developers to leverage\\nGemma’s chat and code capabilities to support\\ntheir own beneficial applications, while allowing\\nforcustomfine-tuningtospecializethemodel’sca-\\npabilities for specific use cases. To ensure Gemma\\nsupports a wide range of developer needs, we are\\nalso releasing two model sizes to optimally sup-\\nportdifferentenvironments,andhavemadethese\\nmodels available across a number of platforms\\n(seeKagglefordetails). Providingbroadaccessto\\nGemma in this way should reduce the economic\\nand technical barriers that newer ventures or in-\\ndependent developers face when incorporating\\nthese technologies into their workstreams.\\nAs well as serving developers with our\\ninstruction-tuned models, we have also provided\\naccess to corresponding base pretrained mod-\\nels. By doing so, it is our intention to encourage\\nfurther AI safety research and community inno-\\nvation, providing a wider pool of models avail-\\nable to developers to build on various methods of\\ntransparency and interpretability research that\\nthe community has already benefited from (Pac-\\nchiardi et al., 2023; Zou et al., 2023).\\nRisks\\nIn addition to bringing benefits to the AI devel-\\nopment ecosystem, we are aware that malicious\\nuses of LLMs, such as the creation of deepfake\\nimagery, AI-generated disinformation, and illegal\\nand disturbing material can cause harm on both\\nan individual and institutional levels (Weidinger\\netal.,2021). Moreover,providingaccesstomodel\\nweights, rather than releasing models behind an\\nAPI, raises new challenges for responsible deploy-\\nment.\\nFirst, we cannot prevent bad actors from fine\\ntuning Gemma for malicious intent, despite their\\nuse being subject to Terms of Use that prohibit\\nthe use of Gemma models in ways that contra-\\nvene our Gemma Prohibited Use Policy. However,\\nwe are cognizant that further work is required to\\nbuild more robust mitigation strategies againstintentionalmisuseofopensystems, whichGoogle\\nDeepMindwillcontinuetoexplorebothinternally\\nand in collaboration with the wider AI commu-\\nnity.\\nThe second challenge we face is protecting de-\\nvelopers and downstream users against the un-\\nintended behaviours of open models, including\\ngeneration of toxic language or perpetuation of\\ndiscriminatorysocialharms,modelhallucinations\\nandleakageofpersonallyidentifiableinformation.\\nWhendeployingmodelsbehindanAPI,theserisks\\ncan be reduced via various filtering methods.\\nMitigations\\nWithout this layer of defense for the Gemma fam-\\nily of models, we have endeavoured to safeguard\\nagainst these risks by filtering and measuring bi-\\nases in pre-training data in line with the Gemini\\napproach, assessing safety through standardized\\nAI safety benchmarks, internal red teaming to\\nbetter understand the risks associated with exter-\\nnal use of Gemma, and subjecting the models to\\nrigorous ethics and safety evaluations, the results\\nof which can be seen in 8.\\nWhile we’ve invested significantly in improving\\nthe model, we recognize its limitations. To en-\\nsure transparency for downstream users, we’ve\\npublished a detailed model card to provide re-\\nsearchers with a more comprehensive under-\\nstanding of Gemma.\\nWe have also released a Generative AI Respon-\\nsible Toolkit to support developers to build AI\\nresponsibly. This encompasses a series of assets\\nto help developers design and implement respon-\\nsible AI best practices and keep their own users\\nsafe.\\nThe relative novelty of releasing open weights\\nmodels means new uses, and misuses, of these\\nmodels are still being discovered, which is why\\nGoogle DeepMind is committed to the continuous\\nresearch and development of robust mitigation\\nstrategies alongside future model development.\\n9', metadata={'source': './assets/gemma-report.pdf', 'page': 8}),\n",
       " Document(page_content='Gemma: Open Models Based on Gemini Research and Technology\\nUser: <start_of_turn>user\\nKnock knock.<end_of_turn>\\n<start_of_turn>model\\nModel: Who’s there?<end_of_turn>model\\nUser: <start_of_turn>user\\nGemma.<end_of_turn>\\n<start_of_turn>model\\nModel: Gemma who?<end_of_turn>model\\nTable 4|Example dialogue with user and model\\ncontrol tokens.\\nerences from human raters and trained a reward\\nfunction under the Bradley-Terry model (Bradley\\nand Terry, 1952), similarly to Gemini. The pol-\\nicy was trained to optimize this reward function\\nusing a variant of REINFORCE (Williams, 1992)\\nwith a Kullback–Leibler regularization term to-\\nwards the initially tuned model. Similar to the\\nSFT phase, and in order to tune hyperparame-\\nters and additionally mitigate reward hacking\\n(Amodeietal.,2016;Skalseetal.,2022)werelied\\non a high capacity model as an automatic rater\\nand computed side-by-side comparisons against\\nbaseline models.\\nEvaluation\\nWe evaluate Gemma across a broad range of do-\\nmains, using both automated benchmarks and\\nhuman evaluation.\\nHuman Preference Evaluations\\nIn addition to running standard academic bench-\\nmarks on the finetuned models, we sent final re-\\nlease candidates to human evaluation studies to\\nbe compared against the Mistral v0.2 7B Instruct\\nmodel (Jiang et al., 2023).\\nOn a held-out collection of around 1000\\nprompts oriented toward asking models to follow\\ninstructions across creative writing tasks, coding,\\nand following instructions, Gemma 7B IT has a\\n51.7% positive win rate and Gemma 2B IT has\\na 41.6% win rate over Mistral v0.2 7B Instruct.\\nOn a held-out collection of around 400 prompts\\noriented towards testing basic safety protocols,\\nGemma 7B IT has a 58% win rate, while Gemma\\n2B IT has a 56.5% win rate. We report the corre-\\nsponding numbers in Table 5.Model Safety Instruction Following\\nGemma 7B IT 58% 51.7%\\n95% Conf. Interval [55.9%, 60.1%] [49.6%, 53.8%]\\nWin / Tie / Loss 42.9% / 30.2% / 26.9% 42.5% / 18.4% / 39.1%\\nGemma 2B IT 56.5% 41.6%\\n95% Conf. Interval [54.4%, 58.6%] [39.5%, 43.7%]\\nWin / Tie / Loss 44.8% / 22.9% / 32.3% 32.7% / 17.8% / 49.5%\\nTable 5|Win rate of Gemma models versus Mis-\\ntral 7B v0.2 Instruct with 95% confidence inter-\\nvals. We report breakdowns of wins, ties, and\\nlosses, and we break ties evenly when reporting\\nthe final win rate.\\nAutomated Benchmarks\\nWe measure Gemma models’ performance on do-\\nmains including physical reasoning (Bisk et al.,\\n2019), social reasoning (Sap et al., 2019), ques-\\ntion answering (Clark et al., 2019; Kwiatkowski\\net al., 2019), coding (Austin et al., 2021; Chen\\net al., 2021), mathematics (Cobbe et al., 2021),\\ncommonsense reasoning (Sakaguchi et al., 2019),\\nlanguage modeling (Paperno et al., 2016), read-\\ning comprehension (Joshi et al., 2017), and more.\\nFor most automated benchmarks we use the\\nsame evaluation methodology as in Gemini.\\nSpecifically for those where we report perfor-\\nmance compared with Mistral, we replicated\\nmethodology from the Mistral technical report\\nas closely as possible. These specific benchmarks\\nare: ARC (Clark et al., 2018), CommonsenseQA\\n(Talmor et al., 2019), Big Bench Hard (Suzgun\\net al., 2022), and AGI Eval (English-only) (Zhong\\netal.,2023). Duetorestrictivelicensing, wewere\\nunable to run any evaluations on LLaMA-2 and\\ncite only those metrics previously reported (Tou-\\nvron et al., 2023b).\\nWe compare Gemma 2B and 7B models to sev-\\neral external open-source (OSS) LLMs across a\\nseries of academic benchmarks, reported in Table\\n6.\\nOn MMLU (Hendrycks et al., 2020), Gemma\\n7B outperforms all OSS alternatives at the same\\norsmallerscale; italsooutperformsseverallarger\\nmodels, including LLaMA2 13B. However, human\\nexpert performance is gauged at 89.8% by the\\nbenchmark authors; as Gemini Ultra is the first\\n5', metadata={'source': './assets/gemma-report.pdf', 'page': 4}),\n",
       " Document(page_content='Gemma: Open Models Based on Gemini Research and Technology\\nAssessment\\nUltimately,giventhecapabilitiesoflargersystems\\naccessible within the existing ecosystem, we be-\\nlieve the release of Gemma will have a negligible\\neffect on the overall AI risk portfolio. In light\\nof this, and given the utility of these models for\\nresearch, auditing and downstream product de-\\nvelopment, we are confident that the benefit of\\nGemma to the AI community outweighs the risks\\ndescribed.\\nGoing Forward\\nAs a guiding principle, Google DeepMind strives\\nto adopt assessments and safety mitigations pro-\\nportionate to the potential risks from our mod-\\nels. In this case, although we are confident that\\nGemma models will provide a net benefit to the\\ncommunity, our emphasis on safety stems from\\nthe irreversible nature of this release. As the\\nharms resulting from open models are not yet\\nwell defined, nor does an established evaluation\\nframeworkforsuchmodelsexist,wewillcontinue\\nto follow this precedent and take a measured and\\ncautionary approach to open model development.\\nAs capabilities advance, we may need to explore\\nextended testing, staggered releases or alterna-\\ntive access mechanisms to ensure responsible AI\\ndevelopment.\\nAs the ecosystem evolves, we urge the wider\\nAI community to move beyond simplistic ’open\\nvs. closed’ debates, and avoid either exaggerat-\\ning or minimising potential harms, as we believe\\na nuanced, collaborative approach to risks and\\nbenefits is essential. At Google DeepMind we’re\\ncommittedtodevelopinghigh-qualityevaluations\\nand invite the community to join us in this effort\\nfor a deeper understanding of AI systems.\\nDiscussion and Conclusion\\nWe present Gemma, an openly available family\\nof generative language models for text and code.\\nGemma advances the state of the art of openly\\navailable language model performance, safety,\\nand responsible development.\\nIn particular, we are confident that Gemmamodels will provide a net benefit to the commu-\\nnity given our extensive safety evaluations and\\nmitigations; however, we acknowledge that this\\nrelease is irreversible and the harms resulting\\nfrom open models are not yet well defined, so we\\ncontinue to adopt assessments and safety mitiga-\\ntions proportionate to the potential risks of these\\nmodels. In addition, our models outperform com-\\npetitors on 6 standard safety benchmarks, and in\\nhuman side-by-side evaluations.\\nGemma models improve performance on a\\nbroad range of domains including dialogue, rea-\\nsoning, mathematics, and code generation. Re-\\nsults on MMLU (64.3%) and MBPP (44.4%)\\ndemonstrate both the high performance of\\nGemma, as well as the continued headroom in\\nopenly available LLM performance.\\nBeyond state-of-the-art performance measures\\non benchmark tasks, we are excited to see what\\nnew use-cases arise from the community, and\\nwhat new capabilities emerge as we advance the\\nfield together. We hope that researchers use\\nGemma to accelerate a broad array of research,\\nand we hope that developers create beneficial\\nnew applications, user experiences, and other\\nfunctionality.\\nGemma benefits from many learnings of the\\nGemini model program including code, data,\\narchitecture, instruction tuning, reinforcement\\nlearning from human feedback, and evaluations.\\nAs discussed in the Gemini technical report, we\\nreiterate a non-exhaustive set of limitations to\\nthe use of LLMs. Even with great performance on\\nbenchmark tasks, further research is needed to\\ncreate robust, safe models that reliably perform\\nas intended. Example further research areas in-\\nclude factuality, alignment, complex reasoning,\\nand robustness to adversarial input. As discussed\\nbyGemini, wenotetheneedformorechallenging\\nand robust benchmarks.\\n10', metadata={'source': './assets/gemma-report.pdf', 'page': 9}),\n",
       " Document(page_content='2024-02-21\\nGemma: Open Models Based on Gemini\\nResearch and Technology\\nGemma Team, Google DeepMind1\\n1See Contributions and Acknowledgments section for full author list. Please send correspondence to gemma-1-report@google.com.\\nThisworkintroducesGemma,afamilyoflightweight,state-of-theartopenmodelsbuiltfromtheresearch\\nand technology used to create Gemini models. Gemma models demonstrate strong performance across\\nacademicbenchmarksforlanguageunderstanding, reasoning, andsafety. Wereleasetwosizesofmodels\\n(2 billion and 7 billion parameters), and provide both pretrained and fine-tuned checkpoints. Gemma\\noutperformssimilarlysizedopenmodelson11outof18text-basedtasks, andwepresentcomprehensive\\nevaluations of safety and responsibility aspects of the models, alongside a detailed description of model\\ndevelopment. We believe the responsible release of LLMs is critical for improving the safety of frontier\\nmodels, and for enabling the next wave of LLM innovations.\\nIntroduction\\nWe present Gemma, a family of open models\\nbased on Google’s Gemini models (Gemini Team,\\n2023).\\nWe trained Gemma models on up to 6T to-\\nkens of text, using similar architectures, data,\\nand training recipes as the Gemini model family.\\nLike Gemini, these models achieve strong gener-\\nalist capabilities in text domains, alongside state-\\nof-the-art understanding and reasoning skills at\\nscale. With this work, we release both pre-trained\\nand fine-tuned checkpoints, as well as an open-\\nsource codebase for inference and serving.\\nGemma comes in two sizes: a 7 billion param-\\neter model for efficient deployment and develop-\\nment on GPU and TPU, and a 2 billion param-\\neter model for CPU and on-device applications.\\nEach size is designed to address different compu-\\ntational constraints, applications, and developer\\nrequirements. At each scale, we release raw, pre-\\ntrained checkpoints, as well as checkpoints fine-\\ntuned for dialogue, instruction-following, help-\\nfulness, and safety. We thoroughly evaluate the\\nshortcomingsofourmodelsonasuiteofquantita-\\ntive and qualitative benchmarks. We believe the\\nrelease of both pretrained and fine-tuned check-\\npoints will enable thorough research and inves-\\ntigation into the impact of current instruction-\\ntuning regimes, as well as the development of\\nincreasingly safe and responsible model develop-ment methodologies.\\nGemma advances state-of-the-art performance\\nrelative to comparable-scale (and some larger),\\nopen models (Almazrouei et al., 2023; Jiang\\net al., 2023; Touvron et al., 2023a,b) across a\\nwide range of domains including both automated\\nbenchmarks and human evaluation. Example do-\\nmains include question answering (Clark et al.,\\n2019; Kwiatkowski et al., 2019), commonsense\\nreasoning (Sakaguchi et al., 2019; Suzgun et al.,\\n2022), mathematics and science (Cobbe et al.,\\n2021;Hendrycksetal.,2020),andcoding(Austin\\net al., 2021; Chen et al., 2021). See complete de-\\ntails in the Evaluation section.\\nLike Gemini, Gemma builds on recent work\\non sequence models (Sutskever et al., 2014) and\\ntransformers (Vaswani et al., 2017), deep learn-\\ning methods based on neural networks (LeCun\\net al., 2015), and techniques for large-scale train-\\ning on distributed systems (Barham et al., 2022;\\nDean et al., 2012; Roberts et al., 2023). Gemma\\nalso builds on Google’s long history of open mod-\\nelsandecosystems,includingWord2Vec(Mikolov\\net al., 2013), the Transformer (Vaswani et al.,\\n2017), BERT (Devlin et al., 2018), and T5 (Raffel\\net al., 2019) and T5X (Roberts et al., 2022).\\nWe believe the responsible release of LLMs is\\ncriticalforimprovingthesafetyoffrontiermodels,\\nforensuringequitableaccesstothisbreakthrough\\ntechnology, for enabling rigorous evaluation and\\nanalysis of current techniques, and for enabling\\n©2024 Google DeepMind. All rights reserved', metadata={'source': './assets/gemma-report.pdf', 'page': 0})]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents(\"Gemma\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tool calling + RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemma_llm = MLXPipeline.from_model_id(\n",
    "    \"mlx-community/gemma-1.1-2b-it-4bit\",\n",
    "    pipeline_kwargs={\"max_tokens\": 100, \"temp\": 0.1},\n",
    ")\n",
    "\n",
    "gemma_chat_model = ChatMLX(llm=gemma_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    formatted_docs = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        formatted_doc = f\"<doc id='{i}'>{doc.page_content}</doc>\"\n",
    "        formatted_docs.append(formatted_doc)\n",
    "    return \"\\n\".join(formatted_docs)\n",
    "\n",
    "\n",
    "def create_retriever_chain():\n",
    "    template = \"\"\"Answer the question based only on the following context:\n",
    "            {context}\n",
    "\n",
    "            Question: {question}\n",
    "        \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    chain = (\n",
    "        {\n",
    "            \"context\": retriever | RunnableLambda(format_docs),\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "        | prompt\n",
    "        | gemma_chat_model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return chain\n",
    "\n",
    "\n",
    "def tool_call(function_name, arguments):\n",
    "    if function_name == 'get_weather':\n",
    "\n",
    "        return \"The weather is good!\"\n",
    "\n",
    "    if function_name == 'retriever':\n",
    "        question = arguments['user_query']['question']\n",
    "        retriever_chain = create_retriever_chain()\n",
    "        output = retriever_chain.invoke({\"question\": question})\n",
    "\n",
    "        return output.content\n",
    "\n",
    "    else:\n",
    "        messages = [\n",
    "            HumanMessage(\n",
    "                content=arguments['user_query']['question']\n",
    "            ),\n",
    "        ]\n",
    "        output = gemma_chat_model.invoke(messages)\n",
    "\n",
    "        return output.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_name= router_output[\"tool_call\"][\"function_name\"]\n",
    "args = router_output[\"tool_call\"][\"arguments\"]\n",
    "tool_call_out = tool_call(function_name, args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx_code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
